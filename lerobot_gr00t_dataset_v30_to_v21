#!/usr/bin/env python

Reverse-conversion: convert any LeRobot dataset already pushed to the hub from codebase version 3.0 back to
2.1. This script attempts to undo the structural changes made by the original converter (v2.1 -> v3.0):

- Split consolidated `data` files (`meta`/data files like `file_000.parquet`) back into per-episode
    parquet files under `data/chunk-XXX/episode_XXXXXX.parquet`.
- Split consolidated `videos` files back into per-episode mp4s using timestamps in `meta/episodes`.
- Regenerate legacy `episodes.jsonl`, `tasks.jsonl` and `episodes_stats.jsonl`.
- Update `meta/info.json` to use `v2.1` and restore relevant legacy fields where possible.
- Tag the dataset with `v2.1` and push to the hub.

This is a best-effort reverse; for complex or custom datasets manual checks may be required.

Usage:

```bash
python src/lerobot/datasets/v30/convert_dataset_v21_to_v30.py \
        --repo-id=lerobot/pusht
```

"""

import argparse
import shutil
from pathlib import Path
from typing import Any

import jsonlines
import json
import sys
import os
from pathlib import Path as _Path

# If the package is executed directly (python path/to/this/script.py) the
# repository `src/` folder may not be on sys.path. Try to locate the project
# root and add it so `import lerobot.*` works when running this script.
try:
    import lerobot  # type: ignore
except Exception:
    _curr = _Path(__file__).resolve().parent
    for _ in range(6):
        candidate = _curr / "src"
        if candidate.is_dir() and (candidate / "lerobot").is_dir():
            sys.path.insert(0, str(candidate))
            break
        if (_curr / "lerobot").is_dir():
            sys.path.insert(0, str(_curr))
            break
        _curr = _curr.parent
import numpy as np
import pandas as pd
import pyarrow as pa
import tqdm
from datasets import Dataset, Features, Image
from huggingface_hub import HfApi, snapshot_download
from requests import HTTPError

from lerobot.utils.constants import HF_LEROBOT_HOME
from lerobot.datasets.compute_stats import aggregate_stats
from lerobot.datasets.lerobot_dataset import CODEBASE_VERSION, LeRobotDataset
from lerobot.datasets.utils import (
    DEFAULT_CHUNK_SIZE,
    DEFAULT_DATA_FILE_SIZE_IN_MB,
    DEFAULT_DATA_PATH,
    DEFAULT_VIDEO_FILE_SIZE_IN_MB,
    DEFAULT_VIDEO_PATH,
    LEGACY_EPISODES_PATH,
    LEGACY_EPISODES_STATS_PATH,
    LEGACY_TASKS_PATH,
    cast_stats_to_numpy,
    flatten_dict,
    get_parquet_file_size_in_mb,
    get_parquet_num_frames,
    get_video_size_in_mb,
    load_info,
    update_chunk_file_indices,
    write_episodes,
    write_info,
    write_stats,
    write_tasks,
)
from lerobot.datasets.video_utils import concatenate_video_files, get_video_duration_in_s

V30 = "v3.0"
V21 = "v2.1"


"""
-------------------------
OLD
data/chunk-000/episode_000000.parquet

NEW
data/chunk-000/file_000.parquet
-------------------------
OLD
videos/chunk-000/CAMERA/episode_000000.mp4

NEW
videos/chunk-000/file_000.mp4
-------------------------
OLD
episodes.jsonl
{"episode_index": 1, "tasks": ["Put the blue block in the green bowl"], "length": 266}

NEW
meta/episodes/chunk-000/episodes_000.parquet
episode_index | video_chunk_index | video_file_index | data_chunk_index | data_file_index | tasks | length
-------------------------
OLD
tasks.jsonl
{"task_index": 1, "task": "Put the blue block in the green bowl"}

NEW
meta/tasks/chunk-000/file_000.parquet
task_index | task
-------------------------
OLD
episodes_stats.jsonl

NEW
meta/episodes_stats/chunk-000/file_000.parquet
episode_index | mean | std | min | max
-------------------------
UPDATE
meta/info.json
-------------------------
"""


def load_jsonlines(fpath: Path) -> list[Any]:
    # Robust JSONL loader: some legacy jsonl files contain empty lines or
    # slightly malformed lines that the jsonlines reader will reject. Read
    # line-by-line and skip empty/malformed lines instead of failing hard.
    items: list[Any] = []
    with open(fpath, "r") as f:
        for line in f:
            line = line.strip()
            if not line:
                continue
            try:
                items.append(json.loads(line))
            except Exception:
                # last-resort: try jsonlines for compatibility, but ignore bad lines
                try:
                    with jsonlines.open(fpath, "r") as reader:
                        for obj in reader:
                            items.append(obj)
                except Exception:
                    # give up on this line and continue
                    continue
                break
    return items


def legacy_load_episodes(local_dir: Path) -> dict:
    episodes = load_jsonlines(local_dir / LEGACY_EPISODES_PATH)
    return {item["episode_index"]: item for item in sorted(episodes, key=lambda x: x["episode_index"])}


def legacy_load_episodes_stats(local_dir: Path) -> dict:
    episodes_stats = load_jsonlines(local_dir / LEGACY_EPISODES_STATS_PATH)
    return {
        item["episode_index"]: cast_stats_to_numpy(item["stats"])
        for item in sorted(episodes_stats, key=lambda x: x["episode_index"])
    }


def legacy_load_tasks(local_dir: Path) -> tuple[dict, dict]:
    tasks = load_jsonlines(local_dir / LEGACY_TASKS_PATH)
    tasks = {item["task_index"]: item["task"] for item in sorted(tasks, key=lambda x: x["task_index"])}
    task_to_task_index = {task: task_index for task_index, task in tasks.items()}
    return tasks, task_to_task_index


def convert_tasks(root, new_root):
    """Convert meta/tasks (v3.0 parquet) back to legacy tasks.jsonl format.

    Expected meta layout: `meta/tasks/chunk-000/file_000.parquet` with columns
    `task_index` and `task`.
    """
    # Read all parquet files under meta/tasks
    tasks_parquets = sorted((root / "meta" / "tasks").glob("**/*.parquet"))
    tasks_list = []
    for p in tasks_parquets:
        df = pd.read_parquet(p)
        for _, row in df.iterrows():
            tasks_list.append({"task_index": int(row["task_index"]), "task": str(row["task"])})

    # Write legacy tasks.jsonl
    out_path = new_root / LEGACY_TASKS_PATH
    out_path.parent.mkdir(parents=True, exist_ok=True)
    with jsonlines.open(out_path, "w") as writer:
        for item in sorted(tasks_list, key=lambda x: x["task_index"]):
            writer.write(item)


def concat_data_files(paths_to_cat, new_root, chunk_idx, file_idx, image_keys):
    # (original helper) kept for potential reuse. In reverse conversion we will use
    # splitting functions instead of concatenation.
    dataframes = [pd.read_parquet(file) for file in paths_to_cat]
    concatenated_df = pd.concat(dataframes, ignore_index=True)

    path = new_root / DEFAULT_DATA_PATH.format(chunk_index=chunk_idx, file_index=file_idx)
    path.parent.mkdir(parents=True, exist_ok=True)

    if len(image_keys) > 0:
        schema = pa.Schema.from_pandas(concatenated_df)
        features = Features.from_arrow_schema(schema)
        for key in image_keys:
            features[key] = Image()
        schema = features.arrow_schema
    else:
        schema = None

    concatenated_df.to_parquet(path, index=False, schema=schema)


def split_data_files(root: Path, new_root: Path):
    """Split v3.0 consolidated data files back into per-episode parquet files used in v2.1.

    Uses `meta/episodes` to find for each episode the originating data file and the
    `dataset_from_index`/`dataset_to_index` slice to extract from the consolidated parquet.
    """
    # Load meta/episodes (it is written in v3.0 as parquet chunks)
    meta_episodes_paths = sorted((root / "meta" / "episodes").glob("**/*.parquet"))
    if not meta_episodes_paths:
        return []

    meta_dfs = [pd.read_parquet(p) for p in meta_episodes_paths]
    meta_df = pd.concat(meta_dfs, ignore_index=True)
    meta_df = meta_df.sort_values("episode_index").reset_index(drop=True)

    image_keys = get_image_keys(root)

    # Group by data chunk/file to avoid reopening the same large parquet many times
    grouped = meta_df.groupby(["data/chunk_index", "data/file_index"])
    episodes_metadata = []
    for (chunk_idx, file_idx), group in grouped:
        src_path = root / DEFAULT_DATA_PATH.format(chunk_index=chunk_idx, file_index=file_idx)
        if not src_path.exists():
            raise FileNotFoundError(f"Expected data file {src_path} not found")
        df = pd.read_parquet(src_path)

        for _, row in group.iterrows():
            ep_idx = int(row["episode_index"])
            start = int(row["dataset_from_index"])
            end = int(row["dataset_to_index"])
            ep_df = df.iloc[start:end].reset_index(drop=True)

            # Build legacy path: data/chunk-000/episode_000000.parquet
            out_dir = new_root / "data" / f"chunk-{chunk_idx:03d}"
            out_dir.mkdir(parents=True, exist_ok=True)
            out_file = out_dir / f"episode_{ep_idx:06d}.parquet"

            # Preserve image schema if present
            if len(image_keys) > 0:
                schema = pa.Schema.from_pandas(ep_df)
                features = Features.from_arrow_schema(schema)
                for key in image_keys:
                    features[key] = Image()
                schema = features.arrow_schema
            else:
                schema = None

            ep_df.to_parquet(out_file, index=False, schema=schema)

            episodes_metadata.append({
                "episode_index": ep_idx,
                "tasks": None,  # tasks will be recreated separately
                "length": len(ep_df),
            })

    return episodes_metadata


def convert_data(root: Path, new_root: Path, data_file_size_in_mb: int):
    data_dir = root / "data"
    ep_paths = sorted(data_dir.glob("*/*.parquet"))

    image_keys = get_image_keys(root)

    ep_idx = 0
    chunk_idx = 0
    file_idx = 0
    size_in_mb = 0
    num_frames = 0
    paths_to_cat = []
    episodes_metadata = []
    for ep_path in ep_paths:
        ep_size_in_mb = get_parquet_file_size_in_mb(ep_path)
        ep_num_frames = get_parquet_num_frames(ep_path)
        ep_metadata = {
            "episode_index": ep_idx,
            "data/chunk_index": chunk_idx,
            "data/file_index": file_idx,
            "dataset_from_index": num_frames,
            "dataset_to_index": num_frames + ep_num_frames,
        }
        size_in_mb += ep_size_in_mb
        num_frames += ep_num_frames
        episodes_metadata.append(ep_metadata)
        ep_idx += 1

        if size_in_mb < data_file_size_in_mb:
            paths_to_cat.append(ep_path)
            continue

        if paths_to_cat:
            concat_data_files(paths_to_cat, new_root, chunk_idx, file_idx, image_keys)

        # Reset for the next file
        size_in_mb = ep_size_in_mb
        num_frames = ep_num_frames
        paths_to_cat = [ep_path]

        chunk_idx, file_idx = update_chunk_file_indices(chunk_idx, file_idx, DEFAULT_CHUNK_SIZE)

    # Write remaining data if any
    if paths_to_cat:
        concat_data_files(paths_to_cat, new_root, chunk_idx, file_idx, image_keys)

    return episodes_metadata


def get_video_keys(root):
    info = load_info(root)
    features = info["features"]
    video_keys = [key for key, ft in features.items() if ft["dtype"] == "video"]
    return video_keys


def get_image_keys(root):
    info = load_info(root)
    features = info["features"]
    image_keys = [key for key, ft in features.items() if ft["dtype"] == "image"]
    return image_keys


def convert_videos(root: Path, new_root: Path, video_file_size_in_mb: int):
    # For reverse conversion we will split consolidated video files back into single-episode mp4s
    # using the timestamps stored in meta/episodes. We'll return per-episode video metadata used later
    video_keys = get_video_keys(root)
    if len(video_keys) == 0:
        return None

    video_keys = sorted(video_keys)
    # Load meta episodes to obtain timestamps
    meta_episodes_paths = sorted((root / "meta" / "episodes").glob("**/*.parquet"))
    if not meta_episodes_paths:
        return None
    meta_df = pd.concat([pd.read_parquet(p) for p in meta_episodes_paths], ignore_index=True)
    meta_df = meta_df.sort_values("episode_index").reset_index(drop=True)

    # We'll process each video_key: group by videos/{key}/chunk_index and file_index
    episodic_video_metadata = []
    for video_key in video_keys:
        # For each episode read which merged file it belongs to and the timestamps
        for _, row in meta_df.iterrows():
            ep_idx = int(row["episode_index"])
            chunk_idx = int(row[f"videos/{video_key}/chunk_index"])
            file_idx = int(row[f"videos/{video_key}/file_index"])
            from_t = float(row[f"videos/{video_key}/from_timestamp"])
            to_t = float(row[f"videos/{video_key}/to_timestamp"])

            merged_path = root / DEFAULT_VIDEO_PATH.format(video_key=video_key, chunk_index=chunk_idx, file_index=file_idx)
            if not merged_path.exists():
                raise FileNotFoundError(f"Expected merged video {merged_path} not found")

            # Output legacy path: videos/chunk-000/CAMERA/episode_000000.mp4
            out_dir = new_root / "videos" / f"chunk-{chunk_idx:03d}" / video_key
            out_dir.mkdir(parents=True, exist_ok=True)
            out_file = out_dir / f"episode_{ep_idx:06d}.mp4"

            # Use moviepy to trim (best-effort). If moviepy isn't available this will raise ImportError.
            try:
                from moviepy.editor import VideoFileClip

                with VideoFileClip(str(merged_path)) as clip:
                    sub = clip.subclip(from_t, to_t)
                    sub.write_videofile(str(out_file), audio=False, verbose=False, logger=None)
            except Exception:
                # If trimming fails, fallback to copying the whole merged file (less ideal)
                shutil.copy(str(merged_path), str(out_file))

            episodic_video_metadata.append({
                "episode_index": ep_idx,
                "camera": video_key,
                "path": str(out_file),
            })

    return episodic_video_metadata


def convert_videos_of_camera(root: Path, new_root: Path, video_key: str, video_file_size_in_mb: int):
    # Access old paths to mp4
    videos_dir = root / "videos"
    ep_paths = sorted(videos_dir.glob(f"*/{video_key}/*.mp4"))

    ep_idx = 0
    chunk_idx = 0
    file_idx = 0
    size_in_mb = 0
    duration_in_s = 0.0
    paths_to_cat = []
    episodes_metadata = []
    for ep_path in tqdm.tqdm(ep_paths, desc=f"convert videos of {video_key}"):
        ep_size_in_mb = get_video_size_in_mb(ep_path)
        ep_duration_in_s = get_video_duration_in_s(ep_path)

        # Check if adding this episode would exceed the limit
        if size_in_mb + ep_size_in_mb >= video_file_size_in_mb and len(paths_to_cat) > 0:
            # Size limit would be exceeded, save current accumulation WITHOUT this episode
            concatenate_video_files(
                paths_to_cat,
                new_root
                / DEFAULT_VIDEO_PATH.format(video_key=video_key, chunk_index=chunk_idx, file_index=file_idx),
            )

            # Update episodes metadata for the file we just saved
            for i, _ in enumerate(paths_to_cat):
                past_ep_idx = ep_idx - len(paths_to_cat) + i
                episodes_metadata[past_ep_idx][f"videos/{video_key}/chunk_index"] = chunk_idx
                episodes_metadata[past_ep_idx][f"videos/{video_key}/file_index"] = file_idx

            # Move to next file and start fresh with current episode
            chunk_idx, file_idx = update_chunk_file_indices(chunk_idx, file_idx, DEFAULT_CHUNK_SIZE)
            size_in_mb = 0
            duration_in_s = 0.0
            paths_to_cat = []

        # Add current episode metadata
        ep_metadata = {
            "episode_index": ep_idx,
            f"videos/{video_key}/chunk_index": chunk_idx,  # Will be updated when file is saved
            f"videos/{video_key}/file_index": file_idx,  # Will be updated when file is saved
            f"videos/{video_key}/from_timestamp": duration_in_s,
            f"videos/{video_key}/to_timestamp": duration_in_s + ep_duration_in_s,
        }
        episodes_metadata.append(ep_metadata)

        # Add current episode to accumulation
        paths_to_cat.append(ep_path)
        size_in_mb += ep_size_in_mb
        duration_in_s += ep_duration_in_s
        ep_idx += 1

    # Write remaining videos if any
    if paths_to_cat:
        concatenate_video_files(
            paths_to_cat,
            new_root
            / DEFAULT_VIDEO_PATH.format(video_key=video_key, chunk_index=chunk_idx, file_index=file_idx),
        )

        # Update episodes metadata for the final file
        for i, _ in enumerate(paths_to_cat):
            past_ep_idx = ep_idx - len(paths_to_cat) + i
            episodes_metadata[past_ep_idx][f"videos/{video_key}/chunk_index"] = chunk_idx
            episodes_metadata[past_ep_idx][f"videos/{video_key}/file_index"] = file_idx

    return episodes_metadata


def generate_episode_metadata_dict(
    episodes_legacy_metadata, episodes_metadata, episodes_stats, episodes_videos=None
):
    num_episodes = len(episodes_metadata)
    episodes_legacy_metadata_vals = list(episodes_legacy_metadata.values())
    episodes_stats_vals = list(episodes_stats.values())
    episodes_stats_keys = list(episodes_stats.keys())

    for i in range(num_episodes):
        ep_legacy_metadata = episodes_legacy_metadata_vals[i]
        ep_metadata = episodes_metadata[i]
        ep_stats = episodes_stats_vals[i]

        ep_ids_set = {
            ep_legacy_metadata["episode_index"],
            ep_metadata["episode_index"],
            episodes_stats_keys[i],
        }

        # In reverse mode episodes_videos may be a list of video segments (one per camera)
        if episodes_videos is None:
            ep_video = {}
        else:
            # episodes_videos might be a list of dicts with camera entries; build a flat map
            ep_video_items = [v for v in episodes_videos if v["episode_index"] == i]
            ep_video = {}
            for v in ep_video_items:
                ep_video_key = f"videos/{v['camera']}/path"
                ep_video[ep_video_key] = v["path"]
            if ep_video_items:
                ep_ids_set.add(ep_video_items[0]["episode_index"])

        if len(ep_ids_set) != 1:
            raise ValueError(f"Number of episodes is not the same ({ep_ids_set}).")

        ep_dict = {**ep_metadata, **ep_video, **ep_legacy_metadata, **flatten_dict({"stats": ep_stats})}
        ep_dict["meta/episodes/chunk_index"] = 0
        ep_dict["meta/episodes/file_index"] = 0
        yield ep_dict


def convert_episodes_metadata(root, new_root, episodes_metadata, episodes_video_metadata=None):
    # We need to generate legacy episodes.jsonl and episodes_stats.jsonl from meta/episodes
    meta_episodes_paths = sorted((root / "meta" / "episodes").glob("**/*.parquet"))
    if not meta_episodes_paths:
        raise FileNotFoundError("meta/episodes parquet files not found")
    meta_df = pd.concat([pd.read_parquet(p) for p in meta_episodes_paths], ignore_index=True)
    meta_df = meta_df.sort_values("episode_index").reset_index(drop=True)

    # episodes.jsonl expects entries like {"episode_index": int, "tasks": [...], "length": int}
    legacy_eps_path = new_root / LEGACY_EPISODES_PATH
    legacy_eps_path.parent.mkdir(parents=True, exist_ok=True)
    with jsonlines.open(legacy_eps_path, "w") as writer:
        for _, row in meta_df.iterrows():
            tasks_val = row.get("tasks", None)
            # Convert numpy arrays to lists for JSON serialization
            if isinstance(tasks_val, np.ndarray):
                tasks_val = tasks_val.tolist()
            writer.write({
                "episode_index": int(row["episode_index"]),
                "tasks": tasks_val,
                "length": int(row.get("length", 0)),
            })

    # episodes_stats.jsonl: we expect `stats` flattened in meta; reconstruct if available
    stats_path = new_root / LEGACY_EPISODES_STATS_PATH
    stats_path.parent.mkdir(parents=True, exist_ok=True)
    with jsonlines.open(stats_path, "w") as writer:
        for _, row in meta_df.iterrows():
            stats_raw = row.get("stats", None)
            # Convert numpy arrays/objects to JSON-serializable format
            if isinstance(stats_raw, np.ndarray):
                stats_raw = stats_raw.tolist()
            elif isinstance(stats_raw, dict):
                # If it's a dict with nested numpy arrays, convert each value
                stats_raw = {k: (v.tolist() if isinstance(v, np.ndarray) else v) for k, v in stats_raw.items()}
            writer.write({"episode_index": int(row["episode_index"]), "stats": stats_raw})

    # tasks
    convert_tasks(root, new_root)

    # Data and videos should already be split by other steps


def generate_modality_json(root: Path, new_root: Path):
    """Generate or copy modality.json.

    The modality file maps feature keys to modality groups (state, action, video, etc).
    First tries to copy from source dataset if it exists, otherwise generates a basic version
    by categorizing features by type and key patterns.
    """
    # First, check if source has modality.json at meta/modality.json
    source_modality_path = root / "meta" / "modality.json"
    if source_modality_path.exists():
        dest_modality_path = new_root / "meta" / "modality.json"
        dest_modality_path.parent.mkdir(parents=True, exist_ok=True)
        shutil.copy(source_modality_path, dest_modality_path)
        print(f"✓ Copied modality.json from source dataset")
        return

    # If no source modality.json, try to generate from features
    try:
        info = load_info(root)
    except Exception as e:
        print(f"Warning: Could not load info.json to generate modality.json: {e}")
        return

    features = info.get("features", {})
    if not features:
        print("Warning: No features found in info.json, skipping modality.json generation")
        return

    modality = {
        "state": {},
        "action": {},
        "video": {},
        "image": {},
        "annotation": {},  # Add annotation category
    }

    # Categorize features by type and key prefix
    for key, ft in features.items():
        dtype = ft.get("dtype", "")

        if key.startswith("action"):
            modality["action"][key] = {"original_key": key}
        elif key.startswith("observation.state") or key.startswith("state"):
            modality["state"][key] = {"original_key": key}
        elif dtype == "video" or "video" in key.lower():
            # Extract camera name from key like "observation.images.front"
            camera_name = key.split(".")[-1] if "." in key else key
            modality["video"][camera_name] = {"original_key": key}
        elif dtype == "image" or "image" in key.lower():
            # Extract image name from key like "observation.images.front"
            image_name = key.split(".")[-1] if "." in key else key
            modality["image"][image_name] = {"original_key": key}
        elif "task_index" in key or "task" in key.lower() or "annotation" in key.lower() or "language" in key.lower():
            # Map task/annotation/language fields to annotation modality
            modality["annotation"][key] = {"original_key": key}

    # Remove empty categories
    modality = {k: v for k, v in modality.items() if v}

    if not modality:
        print("Warning: No features could be categorized into modality groups")
        return

    # Write to file
    modality_path = new_root / "meta" / "modality.json"
    modality_path.parent.mkdir(parents=True, exist_ok=True)
    with open(modality_path, "w") as f:
        json.dump(modality, f, indent=4)
    print(f"✓ Generated {modality_path}")


def convert_info(root, new_root, data_file_size_in_mb, video_file_size_in_mb):
    info = load_info(root)
    # Build a v2.1-style info.json using available meta information (best-effort)
    new_info = dict(info)  # shallow copy
    new_info["codebase_version"] = V21

    # Try to compute totals from meta/episodes if available
    meta_episodes_paths = sorted((root / "meta" / "episodes").glob("**/*.parquet"))
    if meta_episodes_paths:
        meta_df = pd.concat([pd.read_parquet(p) for p in meta_episodes_paths], ignore_index=True)
        meta_df = meta_df.sort_values("episode_index").reset_index(drop=True)
        total_episodes = int(len(meta_df))
        if "length" in meta_df.columns:
            total_frames = int(meta_df["length"].sum())
        elif "dataset_to_index" in meta_df.columns and "dataset_from_index" in meta_df.columns:
            total_frames = int((meta_df["dataset_to_index"] - meta_df["dataset_from_index"]).sum())
        else:
            total_frames = int(info.get("total_frames", 0))

        if "data/chunk_index" in meta_df.columns:
            total_chunks = int(meta_df["data/chunk_index"].nunique())
        else:
            total_chunks = int(info.get("total_chunks", 1))
    else:
        total_episodes = int(info.get("total_episodes", 0))
        total_frames = int(info.get("total_frames", 0))
        total_chunks = int(info.get("total_chunks", 0) or 0)

    # Tasks: try to count meta/tasks parquet rows
    tasks_parquets = sorted((root / "meta" / "tasks").glob("**/*.parquet"))
    if tasks_parquets:
        total_tasks = sum(len(pd.read_parquet(p)) for p in tasks_parquets)
    elif (root / LEGACY_TASKS_PATH).exists():
        total_tasks = len(load_jsonlines(root / LEGACY_TASKS_PATH))
    else:
        total_tasks = int(info.get("total_tasks", 0))

    # Videos: estimate as number of cameras * episodes
    video_keys = get_video_keys(root)
    if video_keys:
        total_videos = int(total_episodes * len(video_keys))
    else:
        total_videos = int(info.get("total_videos", 0))

    # Put legacy-style fields
    new_info["total_episodes"] = total_episodes
    new_info["total_frames"] = total_frames
    new_info["total_tasks"] = total_tasks
    new_info["total_videos"] = total_videos
    new_info["total_chunks"] = total_chunks

    # Legacy paths and sizes
    new_info["data_path"] = "data/chunk-{episode_chunk:03d}/episode_{episode_index:06d}.parquet"
    new_info["video_path"] = (
        "videos/chunk-{episode_chunk:03d}/{video_key}/episode_{episode_index:06d}.mp4" if video_keys else None
    )
    new_info["data_files_size_in_mb"] = data_file_size_in_mb
    new_info["video_files_size_in_mb"] = video_file_size_in_mb

    # fps should be integer in legacy format
    new_info["fps"] = int(info.get("fps", 30))

    # Keep features as-is but ensure shapes are tuples (load_info already did that)
    write_info(new_info, new_root)


def convert_dataset(
    repo_id: str,
    branch: str | None = None,
    data_file_size_in_mb: int | None = None,
    video_file_size_in_mb: int | None = None,
    push_to_hub: bool = False,
):
    root = HF_LEROBOT_HOME / repo_id
    old_root = HF_LEROBOT_HOME / f"{repo_id}_old"
    new_root = HF_LEROBOT_HOME / f"{repo_id}_v21"

    if data_file_size_in_mb is None:
        data_file_size_in_mb = DEFAULT_DATA_FILE_SIZE_IN_MB
    if video_file_size_in_mb is None:
        video_file_size_in_mb = DEFAULT_VIDEO_FILE_SIZE_IN_MB

    if old_root.is_dir() and root.is_dir():
        shutil.rmtree(str(root))
        shutil.move(str(old_root), str(root))

    if new_root.is_dir():
        shutil.rmtree(new_root)

    # If dataset folder already exists locally, don't download from the hub.
    if not root.exists():
        snapshot_download(
            repo_id,
            repo_type="dataset",
            revision=V30,
            local_dir=root,
        )

    convert_info(root, new_root, data_file_size_in_mb, video_file_size_in_mb)
    generate_modality_json(root, new_root)
    convert_tasks(root, new_root)
    # Split consolidated v3.0 data files back into per-episode files (v2.1 layout)
    episodes_metadata = split_data_files(root, new_root)
    episodes_videos_metadata = convert_videos(root, new_root, video_file_size_in_mb)
    convert_episodes_metadata(root, new_root, episodes_metadata, episodes_videos_metadata)

    shutil.move(str(root), str(old_root))
    shutil.move(str(new_root), str(root))

    if push_to_hub:
        hub_api = HfApi()
        try:
            hub_api.delete_tag(repo_id, tag=CODEBASE_VERSION, repo_type="dataset")
        except HTTPError as e:
            print(f"tag={CODEBASE_VERSION} probably doesn't exist. Skipping exception ({e})")
            pass
        hub_api.delete_files(
            delete_patterns=["data/chunk*/episode_*", "meta/*.jsonl", "videos/chunk*"],
            repo_id=repo_id,
            revision=branch,
            repo_type="dataset",
        )
        hub_api.create_tag(repo_id, tag=CODEBASE_VERSION, revision=branch, repo_type="dataset")

        LeRobotDataset(repo_id).push_to_hub()


if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument(
        "--repo-id",
        type=str,
        required=True,
        help="Repository identifier on Hugging Face: a community or a user name `/` the name of the dataset "
        "(e.g. `lerobot/pusht`, `cadene/aloha_sim_insertion_human`).",
    )
    parser.add_argument(
        "--branch",
        type=str,
        default=None,
        help="Repo branch to push your dataset. Defaults to the main branch.",
    )
    parser.add_argument(
        "--data-file-size-in-mb",
        type=int,
        default=None,
        help="File size in MB. Defaults to 100 for data and 500 for videos.",
    )
    parser.add_argument(
        "--video-file-size-in-mb",
        type=int,
        default=None,
        help="File size in MB. Defaults to 100 for data and 500 for videos.",
    )
    parser.add_argument(
        "--push-to-hub",
        action="store_true",
        default=False,
        help="If set, push the converted dataset to the HuggingFace Hub (default: False).",
    )

    args = parser.parse_args()
    # Pass push_to_hub explicitly (convert_dataset signature expects it)
    params = vars(args)
    push = params.pop("push_to_hub", False)
    convert_dataset(**params, push_to_hub=push)
